{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "cfg = {\n",
    "    'VGG9':  [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input_size = 32\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.n_maps = cfg[vgg_name][-2]\n",
    "        self.fc = self._make_fc_layers()\n",
    "        self.classifier = nn.Linear(self.n_maps, 10)\n",
    "\n",
    "    def forward(self, x, return_feat=False):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        out = self.classifier(x)\n",
    "        if return_feat:\n",
    "            return out, x\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def _make_fc_layers(self):\n",
    "        layers = []\n",
    "        layers += [nn.Linear(self.n_maps*self.input_size*self.input_size, self.n_maps),\n",
    "                   nn.BatchNorm1d(self.n_maps),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "                self.input_size = self.input_size // 2\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def VGG9():\n",
    "    return VGG('VGG9')\n",
    "\n",
    "def VGG16():\n",
    "    return VGG('VGG16')\n",
    "\n",
    "def VGG19():\n",
    "    return VGG('VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded 256 training 256 testing\n",
      "Model loaded\n",
      "Test loss calculated 601.4557495117188\n",
      "Test accuracy is 0.12890625\n",
      "Train loss calculated 603.4698\n",
      "Train accuracy is 0.10546875\n",
      "69\n",
      "Squared euclidian norm is calculated 3.388254138469054\n",
      "Largest eigenvalue is 11.022581\n",
      "Trace is 225.47794\n",
      "Fisher Rao norm is 2.311321004221044\n",
      "PacBayes flatness 0.25001525948758285\n",
      "PacBayes orig 516.326416015625\n",
      "got hessian\n",
      "Neuronwise tracial measure is 78.11812272504903\n",
      "Neuronwise max eigenvalue measure is 25.68762469291687\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trained_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5660/2325375454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Neuronwise max eigenvalue measure is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxeigen_nm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrained_net\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     outp.write(\n\u001b[1;32m    133\u001b[0m         \"# train_loss \\t test_loss \\t weights_norm \\t max_eignv \\t trace \\t fisher_rao \\t pacbayes_fl \\t pacbayes_orig \\t trace_m \\t meig_m \\n\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_net' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import math\n",
    "# import numpy as np\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "# #parameters to set\n",
    "# nets_dir = 'loss_less_01'\n",
    "# element_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# avg_loss = torch.nn.CrossEntropyLoss()\n",
    "# with_acc = True\n",
    "# filename = nets_dir + \"/measures/reparametrized_comparison_measures_\"\n",
    "# wd = None\n",
    "\n",
    "# set_seeds(1)\n",
    "# testloader = trainloader#load_cifar10(train_batch_size = 50000, test_batch_size = 10000)\n",
    "\n",
    "\n",
    "# input_dim = 32*32*3\n",
    "# output_dim = 10\n",
    "# inputs, labels = iter(trainloader).next()\n",
    "# x_train = inputs.cuda()\n",
    "# y_train = labels.cuda()\n",
    "# inputs, labels = iter(testloader).next()\n",
    "# x_test = inputs.cuda()\n",
    "# y_test = labels.cuda()\n",
    "# train_size = len(x_train)\n",
    "# test_size = len(x_test)\n",
    "# print(\"Data loaded\", train_size, \"training\", test_size, \"testing\")\n",
    "\n",
    "# model.cuda()\n",
    "# print(\"Model loaded\")\n",
    "\n",
    "\n",
    "# ## test data\n",
    "# test_output, test_loss = calculate_loss_on_data(model, element_loss, x_test, y_test)\n",
    "# test_loss *= (train_size//test_size)\n",
    "# print(\"Test loss calculated\", test_loss)\n",
    "# if with_acc:\n",
    "#     acc = softmax_accuracy(test_output, y_test)\n",
    "#     print(\"Test accuracy is\", acc*1.0/len(y_test))\n",
    "\n",
    "# ## train data\n",
    "# train_output, train_loss_overall = calculate_loss_on_data(model, element_loss, x_train, y_train)\n",
    "# print(\"Train loss calculated\", train_loss_overall)\n",
    "# if with_acc:\n",
    "#     acc = softmax_accuracy(train_output, y_train)\n",
    "#     print(\"Train accuracy is\", acc*1.0/len(y_train))\n",
    "#     train_acc = acc*1.0/len(y_train)\n",
    "\n",
    "# train_loss = avg_loss(train_output, y_train)\n",
    "\n",
    "# params = list(model.parameters())\n",
    "# feature_layer_idx = -1\n",
    "# for i in range(len(params)):\n",
    "#     if params[i] is model.classifier.weight:\n",
    "#         feature_layer_idx = i\n",
    "\n",
    "# assert i is not -1\n",
    "# print(i)\n",
    "# feature_layer = list(model.parameters())[feature_layer_idx]\n",
    "\n",
    "# # hessian calculation for the layer of interest\n",
    "# last_layer_jacobian = grad(train_loss, feature_layer, create_graph=True, retain_graph=True)\n",
    "# hessian = []\n",
    "# for n_grd in last_layer_jacobian[0]:\n",
    "#     for w_grd in n_grd:\n",
    "#         drv2 = grad(w_grd, feature_layer, retain_graph=True)\n",
    "#         hessian.append(drv2[0].data.cpu().numpy().flatten())\n",
    "\n",
    "# weights_norm = 0.0\n",
    "# for n in feature_layer.data.cpu().numpy():\n",
    "#     for w in n:\n",
    "#         weights_norm += w**2\n",
    "# print(\"Squared euclidian norm is calculated\", weights_norm)\n",
    "\n",
    "# max_eignv = LA.eigvalsh(hessian)[-1]\n",
    "# print(\"Largest eigenvalue is\", max_eignv)\n",
    "\n",
    "# trace = np.trace(hessian)\n",
    "# print(\"Trace is\", trace)\n",
    "\n",
    "# ## calculate FisherRao norm\n",
    "# # analytical formula for crossentropy loss from Appendix of the original paper\n",
    "# sum_derivatives = 0\n",
    "# m = torch.nn.Softmax(dim=0)\n",
    "# for inp in range(len(train_output)):\n",
    "#     sum_derivatives += \\\n",
    "#         (np.inner(m(train_output[inp]).data.cpu().numpy(), train_output[inp].data.cpu().numpy()) -\n",
    "#             train_output[inp].data.cpu().numpy()[y_train[inp]]) ** 2\n",
    "# fr_norm = math.sqrt(((5 + 1) ** 2) * (1.0 / len(train_output)) * sum_derivatives)\n",
    "# print(\"Fisher Rao norm is\", fr_norm)\n",
    "\n",
    "# # adapted from https://github.com/nitarshan/robust-generalization-measures/blob/master/data/generation/measures.py\n",
    "# sigma = pacbayes_sigma(model, trainloader, train_acc, 42)\n",
    "# weights = get_weights_only(model)\n",
    "# w_vec = get_vec_params(weights)\n",
    "# pacbayes_flat = 1.0 / sigma ** 2\n",
    "# print(\"PacBayes flatness\", pacbayes_flat)\n",
    "# def pacbayes_bound(reference_vec):\n",
    "#     return (reference_vec.norm(p=2) ** 2) / (4 * sigma ** 2) + math.log(train_size / sigma) + 10\n",
    "# pacbayes_orig = pacbayes_bound(w_vec).data.cpu().item()\n",
    "# print(\"PacBayes orig\", pacbayes_orig)\n",
    "# #-----------------------------\n",
    "\n",
    "# # normalization of feature layer\n",
    "# _, activation = model(x_train,return_feat=True)[1].data.cpu().numpy()\n",
    "# activation = np.squeeze(activation)\n",
    "# sigma = np.std(activation, axis=0)\n",
    "\n",
    "# j = 0\n",
    "# for p in model.parameters():\n",
    "#     if feature_layer_idx - 2 == j or feature_layer_idx - 1 == j:\n",
    "#         for i, sigma_i in enumerate(sigma):\n",
    "#             if sigma_i != 0.0:\n",
    "#                 p.data[i] = p.data[i] / sigma_i\n",
    "#     if feature_layer_idx == j:\n",
    "#         for i, sigma_i in enumerate(sigma):\n",
    "#             p.data[:,i] = p.data[:,i] * sigma_i\n",
    "#         feature_layer = p\n",
    "#     j += 1\n",
    "    \n",
    "# train_output, train_loss_overall = calculate_loss_on_data(model, element_loss, x_train, y_train)\n",
    "# train_loss = avg_loss(train_output, y_train)\n",
    "\n",
    "# trace_nm, maxeigen_nm = calculateNeuronwiseHessians_fc_layer(feature_layer, train_loss, wd, normalize = False)\n",
    "# print(\"Neuronwise tracial measure is\", trace_nm)\n",
    "# print(\"Neuronwise max eigenvalue measure is\", maxeigen_nm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_splatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
